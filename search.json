[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oyster genomics for everyone: applications to conservation, management, and aquaculture",
    "section": "",
    "text": "Workshop held at the 2024 Northeast Aquaculture Conference & Exposition (NACE) and the 43rd Milford Aquaculture Seminar (MAS)\nDate: January 10, 2024 Time: 1:00 PM to 4:00 PM Location: Omni Providence Hotel (1 W Exchange St, Providence, RI 02903)\nRoom: Bristol/Kent"
  },
  {
    "objectID": "index.html#location-and-date",
    "href": "index.html#location-and-date",
    "title": "Oyster genomics for everyone: applications to conservation, management, and aquaculture",
    "section": "",
    "text": "Workshop held at the 2024 Northeast Aquaculture Conference & Exposition (NACE) and the 43rd Milford Aquaculture Seminar (MAS)\nDate: January 10, 2024 Time: 1:00 PM to 4:00 PM Location: Omni Providence Hotel (1 W Exchange St, Providence, RI 02903)\nRoom: Bristol/Kent"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Oyster genomics for everyone: applications to conservation, management, and aquaculture",
    "section": "Description",
    "text": "Description\nCome learn about exciting new genomic tools for the eastern oyster that can be used to enhance aquaculture and management. The workshop will feature lightning talks highlighting findings and applications of genomic data in aquaculture, presentations on how to non-destructively sample oysters for DNA and the benefits of incorporating genomic data. The second half of the workshop will be hands-on instruction with genomic data from the eastern oyster breeding array."
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Oyster genomics for everyone: applications to conservation, management, and aquaculture",
    "section": "Agenda",
    "text": "Agenda\n\nEastern Oyster Genomics Workshop\n\n\nStart\nEnd\nEvent\nSpeaker\n\n\n\n\n13:00\n13:10\nWelcome and Introduction\nJon Puritz\n\n\n\n\nPart 1: What can genomic data answer?\n\n\n\n13:10\n13:18\nUnderstanding Oyster Population Connectivity and Adaptation in Narragansett Bay\nAmy Zyck\n\n\n13:20\n13:28\nGenomic and phenotypic variation in wild and selectively bred oysters\nAngel Carrasquillo\n\n\n13:30\n13:40\nDo farm oysters contribute to ecosystem services? Assessing interbreeding between aquaculture strains and native stocks\nYuqing Chen\n\n\n13:42\n13:50\nNo oysters were harmed in the making of this: how to non-destructively sample oyster DNA\nKathryn Markey Lundgren\n\n\n13:50\n14:05\nBreak\n\n\n\n\n\nPart 2: Hands-on workshop with the Eastern Oyster SNP Breeding Array\n\n\n\n14:05\n14:20\nFrom DNA to SNP and what is a SNP anyway?\nJon Puritz\n\n\n14:20\n14:40\nIntroduction to R\nJon Puritz\n\n\n14:40\n15:20\nCalculating Genetic Diversity\nJon Puritz\n\n\n15:20\n16:00\nPrincipal Component Analysis and Discrimininant Analysis\nJon Puritz"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Oyster genomics for everyone: applications to conservation, management, and aquaculture",
    "section": "Support",
    "text": "Support\nSupport for the workshop and participants was provided by RI Sea Grant from award R/1822-2022-104-01 to J. Puritz."
  },
  {
    "objectID": "content/Participants.html",
    "href": "content/Participants.html",
    "title": "Participants",
    "section": "",
    "text": "Participants\n\n\n\n\n\n\n\n\nFirst Name\nLast Name\nAffiliation\n\n\n\n\nSamantha\nPennino\nMIT\n\n\nBrittany\nLewis\nAmerican Farmland Trust\n\n\nMegan\nGuidry\nUniversity of Rhode Island\n\n\nCassandra\nCerasia\nThe University of Rhode Island\n\n\nAmy\nZyck\nUniversity of Rhode Island\n\n\nKathryn\nMarkey Lundgren\nUSDA ARS\n\n\nKatie\nMcFarland\nNOAA\n\n\nKeegan\nHart\nUSDA ARS\n\n\nChristopher\nTeufel\nIsland Creek Oysters\n\n\nSarah\nDonelan\nUmass Dartmouth\n\n\nTirosh\nShapira\nCAT\n\n\nDina\nProestou\nUSDA ARS\n\n\nSamuel\nGurr\nNational Academies & NOAA\n\n\nCarrie\nByron\nUNE\n\n\nLexy\nMcCarty\nCherrystone Aqua-Farms\n\n\nCaitlin\nRandall\nUniversity of Rhode Island\n\n\nAlyssa\nStasse\nUniversity of New Hampshire\n\n\nArun\nVenugopalan\nPostdoc - Woods Hole Oceanographic Inst.\n\n\nYuqing\nChen\nCornell University\n\n\nRuby\nKrasnow\nUniversity of Maine\n\n\nMary\nSullivan\nUSDA ARS\n\n\nJon\nPuritz\nUniversity of Rhode Island\n\n\n\n\n\nPresenters\n\n\n\n\n\n\n\n\nFirst Name\nLast Name\nAffiliation\n\n\n\n\nJon\nPuritz\nUniversity of Rhode Island\n\n\nAmy\nZyck\nUniversity of Rhode Island\n\n\nKathryn\nMarkey Lundgren\nUSDA ARS\n\n\nYuqing\nChen\nCornell University\n\n\nAngel\nCarasaquillo\nUniversity of Rhode Island\n\n\n\n\n\nCareer Roles of Participants"
  },
  {
    "objectID": "content/GettingSetup.html",
    "href": "content/GettingSetup.html",
    "title": "Getting Setup for the Workshop",
    "section": "",
    "text": "The workshop is hands-on data analysis, and in an effort to make the class entirely portable and reproducible, we are providing a Virtual Machine to run the workshop on. A virtual machine is like a small, simplified Operating System that operates within your existing computer and allows for a controlled computing environment.\nAlternatively, if you’re comfortable with R and GitHub, you can likely get this analysis to easily work on your existing computer. We’ll provide instructions for setup using both methods below.\n\n\nFirst, you need to install the program VirtualBox (it’s free and available at https://www.virtualbox.org/wiki/Downloads). Once the software is installed, please download our virtual machine from this link.\nIf you’re a MacOS user with newer Apple Silicon Processors (eg. M1 or M2), you’ll need a developmental version of VirtualBox. See here\nYou can then import the virtual machine following these instructions: 1. Open VirtualBox 2. Click File → Import Appliance\n3. Select the file “EOGW_Linux.ova” that you downloaded from Google Drive\nAfter the machine is up and running, please open terminal and type the following commands:\ncd 2024_NACE_GenomicsWorkshop/Workshop\ngit pull\nThen you should open RStudio from the menu button by typing rstudio in the search bar\n\n\n\nIf you have your own RStudio working environment and access to GitHub, you can setup your own computer to perform the analyses that are part of this workshop. You also need to install the program Plink, and make sure to get version 2.\nOpen Rstudio and type the following in your Console:\ninstall.packages(c(\"ggplot2\", \"vcfR\", \"reshape2\", \"adegenet\", \"hierfstat\", \"pcadapt\", \"tidyr\",\"rmarkdown\", \"knitr\"))\nClone the course repository to your computer:\ngit clone https://github.com/MarineEvoEcoLab/NACE_MAS_Genomics_Workshop.git\nNow, you’re ready to go!"
  },
  {
    "objectID": "content/GettingSetup.html#computer-setup",
    "href": "content/GettingSetup.html#computer-setup",
    "title": "Getting Setup for the Workshop",
    "section": "",
    "text": "The workshop is hands-on data analysis, and in an effort to make the class entirely portable and reproducible, we are providing a Virtual Machine to run the workshop on. A virtual machine is like a small, simplified Operating System that operates within your existing computer and allows for a controlled computing environment.\nAlternatively, if you’re comfortable with R and GitHub, you can likely get this analysis to easily work on your existing computer. We’ll provide instructions for setup using both methods below.\n\n\nFirst, you need to install the program VirtualBox (it’s free and available at https://www.virtualbox.org/wiki/Downloads). Once the software is installed, please download our virtual machine from this link.\nIf you’re a MacOS user with newer Apple Silicon Processors (eg. M1 or M2), you’ll need a developmental version of VirtualBox. See here\nYou can then import the virtual machine following these instructions: 1. Open VirtualBox 2. Click File → Import Appliance\n3. Select the file “EOGW_Linux.ova” that you downloaded from Google Drive\nAfter the machine is up and running, please open terminal and type the following commands:\ncd 2024_NACE_GenomicsWorkshop/Workshop\ngit pull\nThen you should open RStudio from the menu button by typing rstudio in the search bar\n\n\n\nIf you have your own RStudio working environment and access to GitHub, you can setup your own computer to perform the analyses that are part of this workshop. You also need to install the program Plink, and make sure to get version 2.\nOpen Rstudio and type the following in your Console:\ninstall.packages(c(\"ggplot2\", \"vcfR\", \"reshape2\", \"adegenet\", \"hierfstat\", \"pcadapt\", \"tidyr\",\"rmarkdown\", \"knitr\"))\nClone the course repository to your computer:\ngit clone https://github.com/MarineEvoEcoLab/NACE_MAS_Genomics_Workshop.git\nNow, you’re ready to go!"
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File &gt; New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools &gt; Global Options &gt; R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#quarto-documentation",
    "href": "content/customizing.html#quarto-documentation",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "Examples",
    "text": "Examples\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFaye lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  },
  {
    "objectID": "content/add-content.html",
    "href": "content/add-content.html",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#edit-and-add-your-pages",
    "href": "content/add-content.html#edit-and-add-your-pages",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#add-your-pages-the-project",
    "href": "content/add-content.html#add-your-pages-the-project",
    "title": "Customize",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "content/SNP_Analysis.html",
    "href": "content/SNP_Analysis.html",
    "title": "Working with SNP Data",
    "section": "",
    "text": "This actually happens outside of R. A cool feature of RStudio\n\n\n\nplink2 --bfile ../Example_Data/final --recode vcf id-paste=iid --out ../Processed_Data/array --allow-extra-chr --mind 0.1 --geno 0.05 --max-alleles 2 --min-alleles 2 --chr 1-10\n\nHere, we are using the program plink to convert the files we got from the genotyping service (final.ped, final.bed, final.fam). This command has several options:\n--bfile ../Example_Data/final this is saying the input is a binary and ../Example_Data/final is a relative PATH to the data files. In the virtual machine, this could aslo be written as /home/eogwparticipant/NACE_MAS_Genomics_Workshop/Example_Data or ~/NACE_MAS_Genomics_Workshop/Example_Data\n--recode vcf id-paste=iid is telling it to recode the file as a VCF file using the “individual id” from the .fam file as the individual label\n--out array is telling it to name the output files with the prefix “array”\n--allow-extra-chr is a Plink specific setting needed when you’re not working with human data\n\n\n--mind 0.1 This is saying we only want individuals with less than 10% missing data\n--geno 0.05 This is saying we only want SNPs with less than 5% missing data\n--max-alleles 2 --min-alleles 2 This says we only want SNPs with two alleles\n--chr 1-10 This says we only want nuclear SNPs. The array has other markser on it including mtDNA and pathogens, “chr 1-10” means only the SNPs on the 10 oyster genome chromosomes\nIt will help us to have both a .bed file and .vcf later one. The code below does the same as the above, but outputs a .bed file instead\n\nplink2 --bfile ../Example_Data/final --make-bed  --mind 0.1 --geno 0.05 --max-alleles 2 --min-alleles 2 --chr 1-10 --allow-extra-chr --out ../Processed_Data/array"
  },
  {
    "objectID": "content/SNP_Analysis.html#quick-filtering-and-management-of-array-data",
    "href": "content/SNP_Analysis.html#quick-filtering-and-management-of-array-data",
    "title": "Working with SNP Data",
    "section": "",
    "text": "This actually happens outside of R. A cool feature of RStudio\n\n\n\nplink2 --bfile ../Example_Data/final --recode vcf id-paste=iid --out ../Processed_Data/array --allow-extra-chr --mind 0.1 --geno 0.05 --max-alleles 2 --min-alleles 2 --chr 1-10\n\nHere, we are using the program plink to convert the files we got from the genotyping service (final.ped, final.bed, final.fam). This command has several options:\n--bfile ../Example_Data/final this is saying the input is a binary and ../Example_Data/final is a relative PATH to the data files. In the virtual machine, this could aslo be written as /home/eogwparticipant/NACE_MAS_Genomics_Workshop/Example_Data or ~/NACE_MAS_Genomics_Workshop/Example_Data\n--recode vcf id-paste=iid is telling it to recode the file as a VCF file using the “individual id” from the .fam file as the individual label\n--out array is telling it to name the output files with the prefix “array”\n--allow-extra-chr is a Plink specific setting needed when you’re not working with human data\n\n\n--mind 0.1 This is saying we only want individuals with less than 10% missing data\n--geno 0.05 This is saying we only want SNPs with less than 5% missing data\n--max-alleles 2 --min-alleles 2 This says we only want SNPs with two alleles\n--chr 1-10 This says we only want nuclear SNPs. The array has other markser on it including mtDNA and pathogens, “chr 1-10” means only the SNPs on the 10 oyster genome chromosomes\nIt will help us to have both a .bed file and .vcf later one. The code below does the same as the above, but outputs a .bed file instead\n\nplink2 --bfile ../Example_Data/final --make-bed  --mind 0.1 --geno 0.05 --max-alleles 2 --min-alleles 2 --chr 1-10 --allow-extra-chr --out ../Processed_Data/array"
  },
  {
    "objectID": "content/SNP_Analysis.html#import-data-into-r",
    "href": "content/SNP_Analysis.html#import-data-into-r",
    "title": "Working with SNP Data",
    "section": "Import data into R",
    "text": "Import data into R\nNow we’re ready to import that data into R for processing. We will first use the library vcfR to do this\n\nlibrary(vcfR)\n\n\n   *****       ***   vcfR   ***       *****\n   This is vcfR 1.15.0 \n     browseVignettes('vcfR') # Documentation\n     citation('vcfR') # Citation\n   *****       *****      *****       *****\n\nmy_vcf &lt;- read.vcfR(\"../Processed_Data/array.vcf\")\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 15\n  header_line: 16\n  variant count: 60517\n  column count: 54\n\nMeta line 15 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 60517\n  Character matrix gt cols: 54\n  skip: 0\n  nrows: 60517\n  row_num: 0\n\nProcessed variant 1000\nProcessed variant 2000\nProcessed variant 3000\nProcessed variant 4000\nProcessed variant 5000\nProcessed variant 6000\nProcessed variant 7000\nProcessed variant 8000\nProcessed variant 9000\nProcessed variant 10000\nProcessed variant 11000\nProcessed variant 12000\nProcessed variant 13000\nProcessed variant 14000\nProcessed variant 15000\nProcessed variant 16000\nProcessed variant 17000\nProcessed variant 18000\nProcessed variant 19000\nProcessed variant 20000\nProcessed variant 21000\nProcessed variant 22000\nProcessed variant 23000\nProcessed variant 24000\nProcessed variant 25000\nProcessed variant 26000\nProcessed variant 27000\nProcessed variant 28000\nProcessed variant 29000\nProcessed variant 30000\nProcessed variant 31000\nProcessed variant 32000\nProcessed variant 33000\nProcessed variant 34000\nProcessed variant 35000\nProcessed variant 36000\nProcessed variant 37000\nProcessed variant 38000\nProcessed variant 39000\nProcessed variant 40000\nProcessed variant 41000\nProcessed variant 42000\nProcessed variant 43000\nProcessed variant 44000\nProcessed variant 45000\nProcessed variant 46000\nProcessed variant 47000\nProcessed variant 48000\nProcessed variant 49000\nProcessed variant 50000\nProcessed variant 51000\nProcessed variant 52000\nProcessed variant 53000\nProcessed variant 54000\nProcessed variant 55000\nProcessed variant 56000\nProcessed variant 57000\nProcessed variant 58000\nProcessed variant 59000\nProcessed variant 60000\nProcessed variant: 60517\nAll variants processed\n\n\nAbove, we load the library and then use a function to read our vcf and store it as my_vcf"
  },
  {
    "objectID": "content/SNP_Analysis.html#plotting-heterozygosity",
    "href": "content/SNP_Analysis.html#plotting-heterozygosity",
    "title": "Working with SNP Data",
    "section": "Plotting heterozygosity",
    "text": "Plotting heterozygosity\nI’m a fan of ggplot, but in order to use ggplot we need data in a “tidy” format. We can use the package reshape to do this for us\n\nlibrary(reshape2)\nhet_df &lt;- melt(het_results[,c(3:6)], varnames=c('Index', 'Sample'), value.name = 'Heterozygosity', na.rm=TRUE)\n\nNo id variables; using all as measure variables\n\nhead(het_df)\n\n  variable Heterozygosity\n1  Hs_PopA     0.27777778\n2  Hs_PopA     0.32000000\n3  Hs_PopA     0.23111111\n4  Hs_PopA     0.06444444\n5  Hs_PopA     0.23111111\n6  Hs_PopA     0.46444444\n\n\nNow we can easily plot with ggplot. I’m going to use a simple box plot.\n\nlibrary(ggplot2)\np &lt;- ggplot(het_df, aes(x=variable, y=Heterozygosity)) + geom_boxplot(fill=\"#1F78B4\")\np &lt;- p + xlab(\"Population\")\np &lt;- p + ylab(\"Observed Heterozygosity\")\np &lt;- p + theme_bw() + ggtitle(\"Observed Heterozygosity by population and total\")\np"
  },
  {
    "objectID": "content/PCA.html",
    "href": "content/PCA.html",
    "title": "PCA and DAPC",
    "section": "",
    "text": "One important consideration is that we need to deal with linkage in our data set. We will use PCAdapt to help us with this and to illustrate why.\n\n\nFirst, we load in the data and convert our previous array.bed file to pcadapt input\n\nlibrary(pcadapt)\nfilename &lt;- read.pcadapt(\"../Processed_Data/array.bed\", type = \"bed\")\n\n\n\n\nFirst, we can start with a regular PCA. The first line of code below calculates the PCA with 5 PCs and the second line plots the amount of variance that each PC explains.\n\nres &lt;- pcadapt(filename, K = 5)\nplot(res, option = \"screeplot\")\n\n\n\n\nFor this analysis, the idea number of PCs is the one before the variance stars plateauing. Essentially, we’re looking for the “elbow.” Here, this would be 2.\nLet’s plot the PCA. First, we need to recreate the list of population assignments. We’ll do this the same way as the last section.\n\ntable &lt;- read.table(\"../Example_Data/strata\", header = TRUE)\npoplist.names &lt;- table$Population\n\nplot(res, option = \"scores\", pop = poplist.names)\n\n\n\n\nThis might look normal, but you’ll notice that two of the populations are tightly grouped around PC1. We should check too make sure this pattern isn’t being driven by a linkage in the genome. To do this, we can look at the loading scores of the PCs. Loading scores show how much a particular SNP factors into a PC.\n\npar(mfrow = c(2, 2))\nfor (i in 1:4)\n  plot(res$loadings[, i], pch = 19, cex = .3, ylab = paste0(\"Loadings PC\", i))\n\n\n\n\nThese values should be evenly distributed across the genome. You’ll notice that PC1 has a bit of a pattern to it. Let’s zoom in.\n\nplot(res$loadings[, 1], pch = 19, cex = .3, ylab = paste0(\"Loadings PC\", i))\n\n\n\n\nNow, you can definitely see a weird pattern between Index 30,000 and 40,000. This is from several large genomic inversion in the oyster genome. All of those SNPs are highly linked and are driving a large porition of our PCA.\n\n\n\nLinkage Disequilibrium can affect ascertainment of population structure (Abdellaoui et al. 2013). Users analyzing dense data such as SNP Array data should account for LD in their PCAs and PCA-based genome-scans.\nThankfully, PCAdapt has a built in feature that allows us to clump SNPs by LD, basically reducing the data to a more informative subset.\n\nres &lt;- pcadapt(filename, K = 10, LD.clumping = list(size = 500, thr = 0.2))\nplot(res, option = \"screeplot\")\n\n\n\n\nThis changed a bit, and we now have possibly 4 informative PCs. Let’s take a look at the loadings:\n\nres &lt;- pcadapt(filename, K = 4, LD.clumping = list(size = 500, thr = 0.2))\npar(mfrow = c(2, 2))\nfor (i in 1:4)\n  plot(res$loadings[, i], pch = 19, cex = .3, ylab = paste0(\"Loadings PC\", i))\n\n\n\n\nGreat! Now, we no longer see strong patterns in the loading scores. We can proceed.\n\nplot(res, option = \"scores\", pop = poplist.names)\n\n\n\n\nNow, we can see there is much more separation in PopA vs PopB than there was before.\n\n\n\nPCAdapt is great for filtering and for looking for loci under selection (not covered in this tutorial), but there are other packages that are a little more handy for calculating and plotting PCAs and DAPCE. We can filter our VCF to only the post-clumped loci (those with less LD).\nFirst, we will reload our VCF file into R in case you’re starting from scratch, but we made this in the previous tutorial:\n\nlibrary(vcfR)\n\n\n   *****       ***   vcfR   ***       *****\n   This is vcfR 1.15.0 \n     browseVignettes('vcfR') # Documentation\n     citation('vcfR') # Citation\n   *****       *****      *****       *****\n\nmy_vcf &lt;- read.vcfR(\"../Processed_Data/array.vcf\")\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 15\n  header_line: 16\n  variant count: 60517\n  column count: 54\n\nMeta line 15 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 60517\n  Character matrix gt cols: 54\n  skip: 0\n  nrows: 60517\n  row_num: 0\n\nProcessed variant 1000\nProcessed variant 2000\nProcessed variant 3000\nProcessed variant 4000\nProcessed variant 5000\nProcessed variant 6000\nProcessed variant 7000\nProcessed variant 8000\nProcessed variant 9000\nProcessed variant 10000\nProcessed variant 11000\nProcessed variant 12000\nProcessed variant 13000\nProcessed variant 14000\nProcessed variant 15000\nProcessed variant 16000\nProcessed variant 17000\nProcessed variant 18000\nProcessed variant 19000\nProcessed variant 20000\nProcessed variant 21000\nProcessed variant 22000\nProcessed variant 23000\nProcessed variant 24000\nProcessed variant 25000\nProcessed variant 26000\nProcessed variant 27000\nProcessed variant 28000\nProcessed variant 29000\nProcessed variant 30000\nProcessed variant 31000\nProcessed variant 32000\nProcessed variant 33000\nProcessed variant 34000\nProcessed variant 35000\nProcessed variant 36000\nProcessed variant 37000\nProcessed variant 38000\nProcessed variant 39000\nProcessed variant 40000\nProcessed variant 41000\nProcessed variant 42000\nProcessed variant 43000\nProcessed variant 44000\nProcessed variant 45000\nProcessed variant 46000\nProcessed variant 47000\nProcessed variant 48000\nProcessed variant 49000\nProcessed variant 50000\nProcessed variant 51000\nProcessed variant 52000\nProcessed variant 53000\nProcessed variant 54000\nProcessed variant 55000\nProcessed variant 56000\nProcessed variant 57000\nProcessed variant 58000\nProcessed variant 59000\nProcessed variant 60000\nProcessed variant: 60517\nAll variants processed\n\n\n\nld_filtered_vcf &lt;- my_vcf[!is.na(res$loadings[,1]),]\n\nNext, we will convert our vcfR file to a genlight input which can be used by the package adegenet. We will also use the strata and setPop function to enter population level information. Remember, we made the strata_df in the previous tutorial, but we will remake it here just in case.\n\nlibrary(adegenet)\n\nLoading required package: ade4\n\n\n\n   /// adegenet 2.1.10 is loaded ////////////\n\n   &gt; overview: '?adegenet'\n   &gt; tutorials/doc/questions: 'adegenetWeb()' \n   &gt; bug reports/feature requests: adegenetIssues()\n\nmygl &lt;- vcfR2genlight(ld_filtered_vcf,n.cores =2)\n\nstrata&lt;- read.table(\"../Example_Data/strata\", header=TRUE)\nstrata_df &lt;- data.frame(strata)\n\nstrata(mygl) &lt;- strata_df\nsetPop(mygl) &lt;- ~Population\n\nBelow, we use the PCA function of adegent to create a PCA.\n\noyster.pca &lt;- glPca(mygl, nf = 3)\n\nLet’s look at how much variance each PC explains:\n\nbarplot(100*oyster.pca$eig/sum(oyster.pca$eig), col = heat.colors(50), main=\"PCA Variance Explained\")\ntitle(ylab=\"Percent of variance\\nexplained\", line = 2)\ntitle(xlab=\"PC\", line = 1)\n\n\n\n\nWe basically have three informative PCs here. Let’s plot the individuals on the first two PCs:\n\nmyCol &lt;- colorplot(oyster.pca$scores,oyster.pca$scores, transp=TRUE, cex=4)\nabline(h=0,v=0, col=\"grey\")\nadd.scatter.eig(oyster.pca$eig[1:10],2,1,2, posi=\"bottomright\", ratio=.16)\n\n\n\n\nThe plot above is combining information from the first 3 PCs to color dots, and using the first two for x,y coordinates. We can see possibly four groups across all PCs.\nWe can also port this to ggplot and color by population:\n\nlibrary(ggplot2)\noyster.pca.df &lt;- as.data.frame(oyster.pca$scores)\noyster.pca.df$pop &lt;- pop(mygl)\ncols &lt;-c(\"#871a1a\",\"#33A02C\",\"#1F78B4\")\n\np &lt;- ggplot(oyster.pca.df, aes(x=PC1, y=PC2, colour=pop)) \np &lt;- p + geom_point(size=2)\np &lt;- p + stat_ellipse(level = 0.95, size = 1)\np &lt;- p + scale_color_manual(values = cols) \np &lt;- p + geom_hline(yintercept = 0) \np &lt;- p + geom_vline(xintercept = 0) \np &lt;- p + theme_bw()\n\np\n\n\n\n\nSo, we can see that we have some subtle structuring within PopC."
  },
  {
    "objectID": "content/PCA.html#load-data",
    "href": "content/PCA.html#load-data",
    "title": "PCA and DAPC",
    "section": "",
    "text": "First, we load in the data and convert our previous array.bed file to pcadapt input\n\nlibrary(pcadapt)\nfilename &lt;- read.pcadapt(\"../Processed_Data/array.bed\", type = \"bed\")"
  },
  {
    "objectID": "content/PCA.html#examine-the-pcs",
    "href": "content/PCA.html#examine-the-pcs",
    "title": "PCA and DAPC",
    "section": "",
    "text": "First, we can start with a regular PCA. The first line of code below calculates the PCA with 5 PCs and the second line plots the amount of variance that each PC explains.\n\nres &lt;- pcadapt(filename, K = 5)\nplot(res, option = \"screeplot\")\n\n\n\n\nFor this analysis, the idea number of PCs is the one before the variance stars plateauing. Essentially, we’re looking for the “elbow.” Here, this would be 2.\nLet’s plot the PCA. First, we need to recreate the list of population assignments. We’ll do this the same way as the last section.\n\ntable &lt;- read.table(\"../Example_Data/strata\", header = TRUE)\npoplist.names &lt;- table$Population\n\nplot(res, option = \"scores\", pop = poplist.names)\n\n\n\n\nThis might look normal, but you’ll notice that two of the populations are tightly grouped around PC1. We should check too make sure this pattern isn’t being driven by a linkage in the genome. To do this, we can look at the loading scores of the PCs. Loading scores show how much a particular SNP factors into a PC.\n\npar(mfrow = c(2, 2))\nfor (i in 1:4)\n  plot(res$loadings[, i], pch = 19, cex = .3, ylab = paste0(\"Loadings PC\", i))\n\n\n\n\nThese values should be evenly distributed across the genome. You’ll notice that PC1 has a bit of a pattern to it. Let’s zoom in.\n\nplot(res$loadings[, 1], pch = 19, cex = .3, ylab = paste0(\"Loadings PC\", i))\n\n\n\n\nNow, you can definitely see a weird pattern between Index 30,000 and 40,000. This is from several large genomic inversion in the oyster genome. All of those SNPs are highly linked and are driving a large porition of our PCA."
  },
  {
    "objectID": "content/PCA.html#dealing-with-linkage-using-ld-clumping",
    "href": "content/PCA.html#dealing-with-linkage-using-ld-clumping",
    "title": "PCA and DAPC",
    "section": "",
    "text": "Linkage Disequilibrium can affect ascertainment of population structure (Abdellaoui et al. 2013). Users analyzing dense data such as SNP Array data should account for LD in their PCAs and PCA-based genome-scans.\nThankfully, PCAdapt has a built in feature that allows us to clump SNPs by LD, basically reducing the data to a more informative subset.\n\nres &lt;- pcadapt(filename, K = 10, LD.clumping = list(size = 500, thr = 0.2))\nplot(res, option = \"screeplot\")\n\n\n\n\nThis changed a bit, and we now have possibly 4 informative PCs. Let’s take a look at the loadings:\n\nres &lt;- pcadapt(filename, K = 4, LD.clumping = list(size = 500, thr = 0.2))\npar(mfrow = c(2, 2))\nfor (i in 1:4)\n  plot(res$loadings[, i], pch = 19, cex = .3, ylab = paste0(\"Loadings PC\", i))\n\n\n\n\nGreat! Now, we no longer see strong patterns in the loading scores. We can proceed.\n\nplot(res, option = \"scores\", pop = poplist.names)\n\n\n\n\nNow, we can see there is much more separation in PopA vs PopB than there was before."
  },
  {
    "objectID": "content/PCA.html#filter-data-for-pca-and-dapc",
    "href": "content/PCA.html#filter-data-for-pca-and-dapc",
    "title": "PCA and DAPC",
    "section": "",
    "text": "PCAdapt is great for filtering and for looking for loci under selection (not covered in this tutorial), but there are other packages that are a little more handy for calculating and plotting PCAs and DAPCE. We can filter our VCF to only the post-clumped loci (those with less LD).\nFirst, we will reload our VCF file into R in case you’re starting from scratch, but we made this in the previous tutorial:\n\nlibrary(vcfR)\n\n\n   *****       ***   vcfR   ***       *****\n   This is vcfR 1.15.0 \n     browseVignettes('vcfR') # Documentation\n     citation('vcfR') # Citation\n   *****       *****      *****       *****\n\nmy_vcf &lt;- read.vcfR(\"../Processed_Data/array.vcf\")\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 15\n  header_line: 16\n  variant count: 60517\n  column count: 54\n\nMeta line 15 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 60517\n  Character matrix gt cols: 54\n  skip: 0\n  nrows: 60517\n  row_num: 0\n\nProcessed variant 1000\nProcessed variant 2000\nProcessed variant 3000\nProcessed variant 4000\nProcessed variant 5000\nProcessed variant 6000\nProcessed variant 7000\nProcessed variant 8000\nProcessed variant 9000\nProcessed variant 10000\nProcessed variant 11000\nProcessed variant 12000\nProcessed variant 13000\nProcessed variant 14000\nProcessed variant 15000\nProcessed variant 16000\nProcessed variant 17000\nProcessed variant 18000\nProcessed variant 19000\nProcessed variant 20000\nProcessed variant 21000\nProcessed variant 22000\nProcessed variant 23000\nProcessed variant 24000\nProcessed variant 25000\nProcessed variant 26000\nProcessed variant 27000\nProcessed variant 28000\nProcessed variant 29000\nProcessed variant 30000\nProcessed variant 31000\nProcessed variant 32000\nProcessed variant 33000\nProcessed variant 34000\nProcessed variant 35000\nProcessed variant 36000\nProcessed variant 37000\nProcessed variant 38000\nProcessed variant 39000\nProcessed variant 40000\nProcessed variant 41000\nProcessed variant 42000\nProcessed variant 43000\nProcessed variant 44000\nProcessed variant 45000\nProcessed variant 46000\nProcessed variant 47000\nProcessed variant 48000\nProcessed variant 49000\nProcessed variant 50000\nProcessed variant 51000\nProcessed variant 52000\nProcessed variant 53000\nProcessed variant 54000\nProcessed variant 55000\nProcessed variant 56000\nProcessed variant 57000\nProcessed variant 58000\nProcessed variant 59000\nProcessed variant 60000\nProcessed variant: 60517\nAll variants processed\n\n\n\nld_filtered_vcf &lt;- my_vcf[!is.na(res$loadings[,1]),]\n\nNext, we will convert our vcfR file to a genlight input which can be used by the package adegenet. We will also use the strata and setPop function to enter population level information. Remember, we made the strata_df in the previous tutorial, but we will remake it here just in case.\n\nlibrary(adegenet)\n\nLoading required package: ade4\n\n\n\n   /// adegenet 2.1.10 is loaded ////////////\n\n   &gt; overview: '?adegenet'\n   &gt; tutorials/doc/questions: 'adegenetWeb()' \n   &gt; bug reports/feature requests: adegenetIssues()\n\nmygl &lt;- vcfR2genlight(ld_filtered_vcf,n.cores =2)\n\nstrata&lt;- read.table(\"../Example_Data/strata\", header=TRUE)\nstrata_df &lt;- data.frame(strata)\n\nstrata(mygl) &lt;- strata_df\nsetPop(mygl) &lt;- ~Population\n\nBelow, we use the PCA function of adegent to create a PCA.\n\noyster.pca &lt;- glPca(mygl, nf = 3)\n\nLet’s look at how much variance each PC explains:\n\nbarplot(100*oyster.pca$eig/sum(oyster.pca$eig), col = heat.colors(50), main=\"PCA Variance Explained\")\ntitle(ylab=\"Percent of variance\\nexplained\", line = 2)\ntitle(xlab=\"PC\", line = 1)\n\n\n\n\nWe basically have three informative PCs here. Let’s plot the individuals on the first two PCs:\n\nmyCol &lt;- colorplot(oyster.pca$scores,oyster.pca$scores, transp=TRUE, cex=4)\nabline(h=0,v=0, col=\"grey\")\nadd.scatter.eig(oyster.pca$eig[1:10],2,1,2, posi=\"bottomright\", ratio=.16)\n\n\n\n\nThe plot above is combining information from the first 3 PCs to color dots, and using the first two for x,y coordinates. We can see possibly four groups across all PCs.\nWe can also port this to ggplot and color by population:\n\nlibrary(ggplot2)\noyster.pca.df &lt;- as.data.frame(oyster.pca$scores)\noyster.pca.df$pop &lt;- pop(mygl)\ncols &lt;-c(\"#871a1a\",\"#33A02C\",\"#1F78B4\")\n\np &lt;- ggplot(oyster.pca.df, aes(x=PC1, y=PC2, colour=pop)) \np &lt;- p + geom_point(size=2)\np &lt;- p + stat_ellipse(level = 0.95, size = 1)\np &lt;- p + scale_color_manual(values = cols) \np &lt;- p + geom_hline(yintercept = 0) \np &lt;- p + geom_vline(xintercept = 0) \np &lt;- p + theme_bw()\n\np\n\n\n\n\nSo, we can see that we have some subtle structuring within PopC."
  },
  {
    "objectID": "content/PCA.html#dapc-with-inferred-groups",
    "href": "content/PCA.html#dapc-with-inferred-groups",
    "title": "PCA and DAPC",
    "section": "DAPC with inferred groups",
    "text": "DAPC with inferred groups\n\nFinding clusters\nAdegenet has a built in method for inferring population groupings or clusters. It runs interactively, so this code block below won’t run on it’s own. You need to paste it into the Console.\n\ngrp &lt;- find.clusters(mygl, max.n.clust=10)\n\nI’m going to simulate this experience below:\nFirst, you will see a screen like this:  For the purpose of picking clusters, the more PCs the better. Pick any number greater than 40.\nNext screen will be:\n We’d like to see an elbow here, where the lowest point would be the correct number. This doesn’t always work well for subtle structure. I’m going to pick 4 given the results above.\n\ngrp &lt;- find.clusters(mygl, max.n.clust=10, n.pca =100, n.clust =4)\n\nWe can make a simple table to see the assignments:\n\ntable(pop(mygl), grp$grp)\n\n      \n        1  2  3  4\n  PopB  0 15  0  0\n  PopA  0  0  0 15\n  PopC  7  0  8  0\n\n\nThis makes sense given our PCA. Let’s run the DAPC.\n\n\nDiscriminant Analysis\nThis is another interactive function. Run in the console.\n\noyster.dapc &lt;- dapc(mygl, grp$grp)\n\nAgain, I simulate this experience below. First screen is: \nNow this is different than finding clusters. You can overload the analysis with two many PCs. For this, you want to choose the lowes number that gets you &gt; 60% of the variance. I would choose 25 or 30.\nAfter you enter the number of PCs, you’ll see something like this:\n This is picking the number of discriminant functions. Because we put in four groups, there are only three. Truly, only the first two are informative. Let’s pick 2.\n\noyster.dapc &lt;- dapc(mygl, n.pca = 25, n.da = 2, grp$grp)\n\nNow, let’s plot the analysis\n\nscatter(oyster.dapc,col=cols,bg=\"white\", solid=1)\n\n\n\n\nWe can also plot membership probabilities:\n\ncompoplot(oyster.dapc, posi=\"bottom\",txt.leg=paste(\"Cluster\", 1:4), lab=\"\", ncol=4, xlab=\"individuals\")\n\n\n\n\nWe can use ggplot to make this easier to read and put in the populations for each individual:\n\nlibrary(tidyr)\ndapc.df &lt;- as.data.frame(oyster.dapc$posterior)\ndapc.df$pop &lt;- pop(mygl)\ndapc.df$indNames &lt;- rownames(dapc.df)\ncols4 &lt;-c(\"#871a1a\",\"#33A02C\",\"#1F78B4\",\"#FDBF6F\" )\n\ndapc.df &lt;- pivot_longer(dapc.df, -c(pop, indNames))\ncolnames(dapc.df) &lt;- c(\"Original_Pop\",\"Sample\",\"Assigned_Pop\",\"Posterior_membership_probability\")\n\np &lt;- ggplot(dapc.df, aes(x=Sample, y=Posterior_membership_probability, fill=Assigned_Pop))\np &lt;- p + geom_bar(stat='identity') \np &lt;- p + scale_fill_manual(values = cols4) \np &lt;- p + facet_grid(~Original_Pop, scales = \"free\")\np &lt;- p + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))\np &lt;- p + ylab(\"Posterior membership probability\")\np"
  },
  {
    "objectID": "content/PCA.html#dapc-with-original-populations",
    "href": "content/PCA.html#dapc-with-original-populations",
    "title": "PCA and DAPC",
    "section": "DAPC with original populations",
    "text": "DAPC with original populations\nAgain, instead of inferring groups, we can use our original populations.\n\noyster.dapc &lt;- dapc(mygl, n.pca = 25, n.da = 2)\nscatter(oyster.dapc, col = cols, cex = 2, legend = TRUE, clabel = F, posi.leg = \"bottomleft\", scree.pca = TRUE,\n        posi.pca = \"topleft\", cleg = 0.75, posi.da = \"topright\")\n\n\n\n\nLet’s make this better with ggplot\n\ndapc.df &lt;- as.data.frame(oyster.dapc$ind.coord)\ndapc.df$pop &lt;- pop(mygl)\ndapc.df$indNames &lt;- rownames(dapc.df)\n\np &lt;- ggplot(dapc.df, aes(x=LD1, y=LD2, colour=pop)) \np &lt;- p + geom_point(size=2)\np &lt;- p + scale_color_manual(values = cols) \np &lt;- p + geom_hline(yintercept = 0) \np &lt;- p + geom_vline(xintercept = 0) \np &lt;- p + theme_bw()+ xlab(\"DA 1\") + ylab(\"DA 2\")+ ggtitle(\"DAPC with populations as groups\")\n\np"
  },
  {
    "objectID": "content/R_Intro.html",
    "href": "content/R_Intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "R is a statistical programming language that allows users to do advanced computational analysis. It can store a variety of different data types and then perform computation on them.\nHere, we can assign x a value of 5 with:\n\nx&lt;- 5\n\nWe can then ask R to recall that value\n\nx\n\n[1] 5\n\n\nWe can make this more complicated\n\nx &lt;- 5 + 5\nx\n\n[1] 10\n\n\n\nx &lt;- 5\ny &lt;- x + 5\ny\n\n[1] 10\n\n\nR can also store vectors (or a list) of variables. They can be various types: numbers, characters, etc.\n\nvector_a &lt;- c(1,2,3,4,5)\nvector_b &lt;- c(\"a\",\"b\", \"c\")\n\nWe can also use R to access specific values of that array (or list)\n\nvector_a[1]\n\n[1] 1\n\nvector_b[2]\n\n[1] \"b\"\n\n\nThis can be expanded to tables or data frames. R has some example data that’s loaded by default. One is the cars data set\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nThe function summary summarizes the data table for us. We can see there are two columns. We can sample a particular column or row with the index of the table or data frame. For example:\n\nsummary(cars[,1])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    4.0    12.0    15.0    15.4    19.0    25.0 \n\n\nYou’ll notice these values are the same for speed in the output above. This is because we only gave the function the first column of cars\nWe can also subset it using the name of the column\n\nsummary(cars$speed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    4.0    12.0    15.0    15.4    19.0    25.0 \n\n\nAgain, we get the same answer. Instead of summarizing, we can also print the first 10 lines with the function head\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n\n\n\nR also allows programmers to develop and distribute packages which allow reproducible research in nearly any field. We load a package (or also called a library) with the command library\n\nlibrary(ggplot2)\n\nNow we can use the functions that are part of the package! We could spend a whole semester on ggplot, but this package is widely used for plotting data. Here, I’m going to take advantage of more example data: mtcars which has much more data than cars\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\n\nmy_scatplot &lt;- ggplot(mtcars,aes(x=wt,y=mpg,col=cyl)) + geom_point()\nmy_scatplot\n\n\n\n\nHere, we are using ggplot to make a scatter plot. The first command ggplot sets the framework of the plot with a subcommand aes which is short for aethetics. This command tells ggplot to set the variable wt in mtcars as the x variable, mgp as the y variable, and cyl as a variable for coloring.\nWe can do so much more, like customizing lablels by simply “adding” to the plot variable my_scatplot\n\nmy_scatplot + labs(x='Weight (x1000lbs)',y='MPG',colour='Number of\\n Cylinders')\n\n\n\n\nWe have so little time for this workshop, but here is one more example. This is a box plot with customized labels. Hopefully, you can start to see some patterns in how the code works.\n\nmy_boxplot &lt;- ggplot(mtcars,aes(x=cyl,y=mpg, group=cyl, fill=cyl)) + geom_boxplot() + xlab('Cylinders') + ylab('Miles per Gallon')\nmy_boxplot \n\n\n\n\n\n\n\nThere’s lots of different ways to get help in R. Try the examples below, but this code block doesn’t execute on it’s own. Vignettes are minitutorials usually with data and example code, but not all packages have them.\n\nhelp(package = \"pcadapt\")\nhelp(amova)     \n\nbrowseVignettes(package = 'vcfR')\nvignette('vcf_data')\n\n\n\n\nThis intro was done live and is difficult to reproduce, but here’s an annotated image: \n\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It also can be used to render a markdown file (.md) that is suitable for publishing directly on Github. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\n\n\n\nYou can include R code in the document as follows:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\n\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\n\n\n\nRStudio and RMarkdown allow you to run commands through a system terminal. This means we can embed BASH code to run on the machine\n\necho \"Hi, I'm a BASH code output\"\n\nHi, I'm a BASH code output\n\n\n\necho -e \"The Current Directory is $(pwd)\"\n\nThe Current Directory is /home/jpuritz/NACE_MAS_Genomics_Workshop/content"
  },
  {
    "objectID": "content/R_Intro.html#packages-and-loading-them",
    "href": "content/R_Intro.html#packages-and-loading-them",
    "title": "Introduction to R",
    "section": "",
    "text": "R also allows programmers to develop and distribute packages which allow reproducible research in nearly any field. We load a package (or also called a library) with the command library\n\nlibrary(ggplot2)\n\nNow we can use the functions that are part of the package! We could spend a whole semester on ggplot, but this package is widely used for plotting data. Here, I’m going to take advantage of more example data: mtcars which has much more data than cars\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\n\nmy_scatplot &lt;- ggplot(mtcars,aes(x=wt,y=mpg,col=cyl)) + geom_point()\nmy_scatplot\n\n\n\n\nHere, we are using ggplot to make a scatter plot. The first command ggplot sets the framework of the plot with a subcommand aes which is short for aethetics. This command tells ggplot to set the variable wt in mtcars as the x variable, mgp as the y variable, and cyl as a variable for coloring.\nWe can do so much more, like customizing lablels by simply “adding” to the plot variable my_scatplot\n\nmy_scatplot + labs(x='Weight (x1000lbs)',y='MPG',colour='Number of\\n Cylinders')\n\n\n\n\nWe have so little time for this workshop, but here is one more example. This is a box plot with customized labels. Hopefully, you can start to see some patterns in how the code works.\n\nmy_boxplot &lt;- ggplot(mtcars,aes(x=cyl,y=mpg, group=cyl, fill=cyl)) + geom_boxplot() + xlab('Cylinders') + ylab('Miles per Gallon')\nmy_boxplot"
  },
  {
    "objectID": "content/R_Intro.html#getting-help",
    "href": "content/R_Intro.html#getting-help",
    "title": "Introduction to R",
    "section": "",
    "text": "There’s lots of different ways to get help in R. Try the examples below, but this code block doesn’t execute on it’s own. Vignettes are minitutorials usually with data and example code, but not all packages have them.\n\nhelp(package = \"pcadapt\")\nhelp(amova)     \n\nbrowseVignettes(package = 'vcfR')\nvignette('vcf_data')"
  },
  {
    "objectID": "content/R_Intro.html#rstudio",
    "href": "content/R_Intro.html#rstudio",
    "title": "Introduction to R",
    "section": "",
    "text": "This intro was done live and is difficult to reproduce, but here’s an annotated image:"
  },
  {
    "objectID": "content/R_Intro.html#rmarkdown",
    "href": "content/R_Intro.html#rmarkdown",
    "title": "Introduction to R",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It also can be used to render a markdown file (.md) that is suitable for publishing directly on Github. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "content/R_Intro.html#including-code",
    "href": "content/R_Intro.html#including-code",
    "title": "Introduction to R",
    "section": "",
    "text": "You can include R code in the document as follows:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/R_Intro.html#including-plots",
    "href": "content/R_Intro.html#including-plots",
    "title": "Introduction to R",
    "section": "",
    "text": "You can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/R_Intro.html#running-commands-outside-of-r",
    "href": "content/R_Intro.html#running-commands-outside-of-r",
    "title": "Introduction to R",
    "section": "",
    "text": "RStudio and RMarkdown allow you to run commands through a system terminal. This means we can embed BASH code to run on the machine\n\necho \"Hi, I'm a BASH code output\"\n\nHi, I'm a BASH code output\n\n\n\necho -e \"The Current Directory is $(pwd)\"\n\nThe Current Directory is /home/jpuritz/NACE_MAS_Genomics_Workshop/content"
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This website was made with the quarto template from NOAA NMFS Open Science\nThis repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland.\nMany thanks to the NACE/MAS organizers for helping to facilitate this workshop!\n\nEmily Whitmore\nAnne Langston\nChristopher Davis\nLisa Wilke\n\nSpecial thanks to RI Sea Grant for supporting this workshop and participants!\nThanks to all presenters!\nSupport for the workshop and participants was provided by RI Sea Grant from award R/1822-2022-104-01 to J. Puritz."
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/ExtraResources.html",
    "href": "content/ExtraResources.html",
    "title": "Extra Resources",
    "section": "",
    "text": "Tutorial and the source of much of this content LINK\n\n\n\nSWIRL a cool R package that teaches you R in R Quick R by datacamp Short Introduction to R\n\n\n\n\nRopenSci Guide to Reproducible Research LINK\nMore Open Science Guidance! LINK"
  },
  {
    "objectID": "content/ExtraResources.html#overall",
    "href": "content/ExtraResources.html#overall",
    "title": "Extra Resources",
    "section": "",
    "text": "Tutorial and the source of much of this content LINK"
  },
  {
    "objectID": "content/ExtraResources.html#introduction",
    "href": "content/ExtraResources.html#introduction",
    "title": "Extra Resources",
    "section": "",
    "text": "SWIRL a cool R package that teaches you R in R Quick R by datacamp Short Introduction to R"
  },
  {
    "objectID": "content/ExtraResources.html#for-reproducible-research",
    "href": "content/ExtraResources.html#for-reproducible-research",
    "title": "Extra Resources",
    "section": "",
    "text": "RopenSci Guide to Reproducible Research LINK\nMore Open Science Guidance! LINK"
  },
  {
    "objectID": "Extra_Resources/R Resources/Readme.html",
    "href": "Extra_Resources/R Resources/Readme.html",
    "title": "R Resources",
    "section": "",
    "text": "Tutorial and the source of much of this content LINK\n\n\n\nSWIRL a cool R package that teaches you R in R Quick R by datacamp Short Introduction to R\n\n\n\nRopenSci Guide to Reproducible Research LINK\nMore Open Science Guidance! LINK"
  },
  {
    "objectID": "Extra_Resources/R Resources/Readme.html#overall",
    "href": "Extra_Resources/R Resources/Readme.html#overall",
    "title": "R Resources",
    "section": "",
    "text": "Tutorial and the source of much of this content LINK"
  },
  {
    "objectID": "Extra_Resources/R Resources/Readme.html#introduction",
    "href": "Extra_Resources/R Resources/Readme.html#introduction",
    "title": "R Resources",
    "section": "",
    "text": "SWIRL a cool R package that teaches you R in R Quick R by datacamp Short Introduction to R\n\n\n\nRopenSci Guide to Reproducible Research LINK\nMore Open Science Guidance! LINK"
  }
]